{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9776dc0d-e82d-41be-a195-cc82113edc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b3fa89-b3b9-4931-8d62-6f629799e18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Min-Max scaling, also known as normalization, is a data preprocessing technique used to rescale numeric features to a specific range,\n",
    "typically between 0 and 1. It is applied to ensure that all features have the same scale and to prevent any feature from dominating the learning \n",
    "algorithm due to its larger magnitude.\n",
    "\n",
    "The formula for Min-Max scaling is as follows:\n",
    "\n",
    "scaled_value = (x - min) / (max - min)\n",
    "\n",
    "where:\n",
    "\n",
    "x is the original value of the feature.\n",
    "min is the minimum value of the feature in the dataset.\n",
    "max is the maximum value of the feature in the dataset.\n",
    "The result of the scaling formula is that the minimum value of the feature is transformed to 0, the maximum value is transformed to 1, and \n",
    "all other values are linearly scaled between these two extremes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f41f3291-28fd-4282-a6d0-fac5da94e208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data: [10 20 30 15 25]\n",
      "Scaled data: [0.   0.5  1.   0.25 0.75]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "data = np.array([10, 20, 30, 15, 25])\n",
    "\n",
    "# Min-Max scaling\n",
    "min_value = np.min(data)\n",
    "max_value = np.max(data)\n",
    "scaled_data = (data - min_value) / (max_value - min_value)\n",
    "print(\"Original data:\", data)\n",
    "print(\"Scaled data:\", scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9697bf27-ee05-465d-a2b0-f1a1c99dc526",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bf226b-fd58-4ddd-a4ae-b15ac9d78fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "The Unit Vector technique, also known as normalization or feature scaling, is a data preprocessing technique used to transform numeric features \n",
    "into unit vectors, meaning that the resulting vectors have a length or magnitude of 1. It differs from Min-Max scaling in the way it scales the\n",
    "features.\n",
    "\n",
    "The formula for Unit Vector scaling is as follows:\n",
    "\n",
    "scaled_vector = x / ||x||\n",
    "\n",
    "where:\n",
    "\n",
    "x is the original vector.\n",
    "||x|| represents the Euclidean norm or magnitude of the vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23369daa-af78-4f52-abc3-c90865dcd9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orginal data is:  [3 2 1]\n",
      "scaled_data is :  [0.80178373 0.53452248 0.26726124]\n"
     ]
    }
   ],
   "source": [
    "## Pyhthon code\n",
    "import numpy as np\n",
    "data = np.array([3,2,1])\n",
    "manngitude = np.linalg.norm(data)\n",
    "scaled_data = data/magnitude\n",
    "print(\"Orginal data is: \",data)\n",
    "print(\"scaled_data is : \", scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddba8fbc-8134-46c0-b11a-5d61c69797ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6fa003-e524-4d02-a0b0-a804c1e075d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA, which stands for Principal Component Analysis, is a statistical technique used for dimensionality reduction and feature extraction. \n",
    "It transforms a high-dimensional dataset into a lower-dimensional space while preserving the most important information in the data.\n",
    "\n",
    "The main idea behind PCA is to find a new set of orthogonal variables called principal components that capture the maximum variance in the data.\n",
    "These principal components are linear combinations of the original features and are ranked in order of their importance. The first principal component\n",
    "explains the maximum variance in the data, the second principal component explains the second maximum variance, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74787334-98f5-400f-85a3-5d078647a6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "data = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "pca = PCA(n_components = 1)\n",
    "reduced_data = pca.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0de2cbfe-1d0d-4ef9-97ff-6eda2d571448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1, -1],\n",
       "       [-2, -1],\n",
       "       [-3, -2],\n",
       "       [ 1,  1],\n",
       "       [ 2,  1],\n",
       "       [ 3,  2]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "895f98e2-46de-4ab1-8f32-5d61b6a4416a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orginal data dimensions is : (6, 2)\n",
      "Reduced data dimention is :  (6, 1)\n"
     ]
    }
   ],
   "source": [
    "print('Orginal data dimensions is :', data.shape)\n",
    "print('Reduced data dimention is : ', reduced_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d6f9e9-0eaa-4ad9-8e7e-28f43b230bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bd965a-9f26-4ebb-8a43-44580c68c29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA (Principal Component Analysis) can be used as a feature extraction technique. Feature extraction refers to the process of transforming the \n",
    "original features of a dataset into a new set of features that captures the most important information. PCA achieves feature extraction by creating \n",
    "new variables called principal components, which are linear combinations of the original features.\n",
    "\n",
    "The principal components are ranked in order of their importance, with the first principal component capturing the maximum variance in the data, the\n",
    "second principal component capturing the second maximum variance, and so on. By selecting a subset of the principal components, we can effectively\n",
    "reduce the dimensionality of the dataset while preserving the most significant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c9c9cf7-794d-45d1-a1b7-c3557a9f513c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (4, 3)\n",
      "Extracted features shape: (4, 2)\n",
      "Explained variance ratio: [1. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Sample data\n",
    "data = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\n",
    "\n",
    "# Apply PCA for feature extraction\n",
    "pca = PCA(n_components=2)  # Set the desired number of components\n",
    "extracted_features = pca.fit_transform(data)\n",
    "\n",
    "print(\"Original data shape:\", data.shape)\n",
    "print(\"Extracted features shape:\", extracted_features.shape)\n",
    "print(\"Explained variance ratio:\", pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11bef09-09e6-4c12-94d0-07b387cc19bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3a51d8-f96e-4c57-8747-4ad250142910",
   "metadata": {},
   "outputs": [],
   "source": [
    "To use Min-Max scaling for preprocessing the data in a recommendation system for a food delivery service, you would apply this technique to the \n",
    "numerical features such as price, rating, and delivery time. The purpose of Min-Max scaling is to rescale the features to a specific range,\n",
    "typically between 0 and 1, ensuring that all features have the same scale. Here's how you could apply Min-Max scaling to preprocess the data:\n",
    "\n",
    "Identify the numerical features: In your dataset, identify the features that are numerical and require scaling. In this case, it would be price, \n",
    "rating, and delivery time.\n",
    "\n",
    "Determine the minimum and maximum values: Calculate the minimum and maximum values for each of the numerical features. The minimum value represents\n",
    "the smallest value observed for that feature, while the maximum value represents the largest value observed.\n",
    "After applying Min-Max scaling, the numerical features such as price, rating, and delivery time will have values within the range of 0 to 1. This \n",
    "ensures that all features are on a comparable scale and avoids any feature dominating the recommendation system due to its larger magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76f9f0c2-b235-4f16-8b19-c49533b4109b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "data = [[1, 2,5], [0.5, 6,12], [0, 10,15], [1, 18,20]]\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c8a26b3d-10b9-4022-b7a9-893f0e4c647e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orginal data is : [[1, 2, 5], [0.5, 6, 12], [0, 10, 15], [1, 18, 20]]\n",
      "\n",
      "scaled data is : [[1.         0.         0.        ]\n",
      " [0.5        0.25       0.46666667]\n",
      " [0.         0.5        0.66666667]\n",
      " [1.         1.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Orginal data is :\", data)\n",
    "print()\n",
    "print(\"scaled data is :\",scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d710493-3dd3-45b4-8c07-3a37c923ce66",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971e9fa6-fdda-453a-a32c-5eccdfdda6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "In the context of building a model to predict stock prices with a dataset containing many features, such as company financial data and market trends, \n",
    "PCA (Principal Component Analysis) can be used to reduce the dimensionality of the dataset. Here's an explanation of how you could apply PCA for \n",
    "dimensionality reduction:\n",
    "\n",
    "Identify the features: Start by identifying the features in your dataset that are relevant to your prediction task. These features could include \n",
    "financial indicators, market trends, historical prices, or any other factors that may impact stock prices.\n",
    "\n",
    "Standardize the data: Before applying PCA, it's important to standardize the data to have zero mean and unit variance. This step ensures that \n",
    "features with larger magnitudes do not dominate the PCA process. You can use techniques like Z-score normalization or Min-Max scaling to standardize \n",
    "the numerical features in your dataset.\n",
    "\n",
    "Apply PCA: Once the data is standardized, you can apply PCA to the dataset. PCA will transform the original high-dimensional feature space into a \n",
    "lower-dimensional space by creating new orthogonal variables called principal components. These principal components capture the maximum variance in \n",
    "the data.\n",
    "\n",
    "Determine the number of components: Decide on the number of principal components to retain in the reduced dimensionality. This decision can be based \n",
    "on the explained variance ratio, which indicates the proportion of variance explained by each principal component. You can examine the cumulative \n",
    "explained variance ratio and select the number of components that explain a significant portion of the total variance, typically above a certain \n",
    "threshold (e.g., 80% or 90%).\n",
    "\n",
    "Transform the data: Finally, transform the original dataset into the reduced dimensionality by selecting the desired number of principal components.\n",
    "This transformation can be done using the PCA model's transform method.\n",
    "\n",
    "Model training and evaluation: With the reduced-dimensional dataset, you can proceed to train and evaluate your stock price prediction model using \n",
    "the transformed features. The reduced dimensionality obtained through PCA can help mitigate the curse of dimensionality, improve computational \n",
    "efficiency, and potentially enhance the model's performance by focusing on the most informative components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf5af3a-70b4-48f6-9ea1-0fa379b2f8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6e419941-1572-46a3-955f-f3752b36420a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "data = [[1, 5, 10, 15, 20]]\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ccafa6b1-3e01-427c-9e11-3f4854565f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orginal data is : [[1, 5, 10, 15, 20]]\n",
      "\n",
      "scaled data is : [[0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Orginal data is :\", data)\n",
    "print()\n",
    "print(\"scaled data is :\",scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4daad06-2b7a-4dd9-af22-c01fdf3deb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a3db46d5-c126-4358-89ff-5c7898fd2bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance ratio: [7.75323857e-01 1.92470848e-01 3.22052946e-02 4.09017810e-33]\n",
      "Cumulative explained variance: [0.77532386 0.96779471 1.         1.        ]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "data = pd.DataFrame({'height':[170,175,160,180],\n",
    "                   'weight':[65,68,55,70],\n",
    "                   'age':[30,35,28,40],\n",
    "                 'gender':[1,0,1,1] ,\n",
    "                  'blood_pressure':[120,130,110,125]})\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "pca = PCA()\n",
    "pca.fit(scaled_data)\n",
    "\n",
    "# Compute explained variance ratio\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# Compute cumulative explained variance\n",
    "cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "print(\"Explained variance ratio:\", explained_variance_ratio)\n",
    "print(\"Cumulative explained variance:\", cumulative_variance_ratio)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6868fcb-fdcc-45b9-b891-89e1897ffad6",
   "metadata": {},
   "outputs": [],
   "source": [
    " you will see the explained variance ratio for each principal component and the cumulative explained variance at each step. Based on the cumulative\n",
    "    explained variance, you can make a decision on the number of principal components to retain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "29855fc8-2d51-44e3-8fb7-18ee218c637a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 12\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ef154c-81c1-4d8a-9b34-cea8d6809caf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
